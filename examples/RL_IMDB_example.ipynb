{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import gym_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data and scale to input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "# Parameters\n",
    "max_features = 10000\n",
    "maxlen = 100\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to get same len in all samples\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions: Discrete(2)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, None, 128)         1280000   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, None, 256)         263168    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 20)                5140      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 1,548,350\n",
      "Trainable params: 1,548,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 000/030 | Loss 70.3232 | Tot reward x episode 533 \n",
      "Epoch 001/030 | Loss 70.9260 | Tot reward x episode 545 \n",
      "Epoch 002/030 | Loss 63.8965 | Tot reward x episode 651 \n",
      "Epoch 003/030 | Loss 59.3891 | Tot reward x episode 705 \n",
      "Epoch 004/030 | Loss 53.2079 | Tot reward x episode 770 \n",
      "Epoch 005/030 | Loss 48.2950 | Tot reward x episode 801 \n",
      "Epoch 006/030 | Loss 45.9146 | Tot reward x episode 801 \n",
      "Epoch 007/030 | Loss 47.3649 | Tot reward x episode 800 \n",
      "Epoch 008/030 | Loss 46.6619 | Tot reward x episode 788 \n",
      "Epoch 009/030 | Loss 49.8787 | Tot reward x episode 802 \n",
      "Epoch 010/030 | Loss 44.2103 | Tot reward x episode 809 \n",
      "Epoch 011/030 | Loss 42.1041 | Tot reward x episode 830 \n",
      "Epoch 012/030 | Loss 39.7995 | Tot reward x episode 826 \n",
      "Epoch 013/030 | Loss 41.0004 | Tot reward x episode 820 \n",
      "Epoch 014/030 | Loss 42.8702 | Tot reward x episode 828 \n",
      "Epoch 015/030 | Loss 44.5653 | Tot reward x episode 835 \n",
      "Epoch 016/030 | Loss 42.9538 | Tot reward x episode 819 \n",
      "Epoch 017/030 | Loss 41.6971 | Tot reward x episode 849 \n",
      "Epoch 018/030 | Loss 39.2457 | Tot reward x episode 834 \n",
      "Epoch 019/030 | Loss 39.4069 | Tot reward x episode 846 \n",
      "Epoch 020/030 | Loss 40.7407 | Tot reward x episode 833 \n",
      "Epoch 021/030 | Loss 41.9003 | Tot reward x episode 834 \n",
      "Epoch 022/030 | Loss 38.3392 | Tot reward x episode 865 \n",
      "Epoch 023/030 | Loss 35.3667 | Tot reward x episode 839 \n",
      "Epoch 024/030 | Loss 37.5731 | Tot reward x episode 848 \n",
      "Epoch 025/030 | Loss 38.2434 | Tot reward x episode 844 \n",
      "Epoch 026/030 | Loss 44.7431 | Tot reward x episode 850 \n",
      "Epoch 027/030 | Loss 37.6991 | Tot reward x episode 849 \n",
      "Epoch 028/030 | Loss 34.1164 | Tot reward x episode 857 \n",
      "Epoch 029/030 | Loss 37.9420 | Tot reward x episode 874 \n"
     ]
    }
   ],
   "source": [
    "X = x_train\n",
    "y = y_train\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "#input_shape = (maxlen,1)\n",
    "\n",
    "\n",
    " # Initialization of the enviroment\n",
    "env = gym.make('gym_classification:RLClassification-v0')\n",
    "\n",
    "# Fill values\n",
    "env.init_dataset(X,y,batch_size=batch_size) #,output_shape=input_shape\n",
    "\n",
    "\n",
    "# RL parameters\n",
    "valid_actions = env.action_space\n",
    "num_actions = valid_actions.n\n",
    "print(\"Actions: {}\".format(valid_actions))\n",
    "epsilon = .1  # exploration\n",
    "num_episodes = 30\n",
    "iterations_episode = 100\n",
    "\n",
    "decay_rate = 0.99\n",
    "gamma = 0.001\n",
    "\n",
    "\n",
    "# Network arquitecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(Bidirectional(LSTM(128, recurrent_dropout=0.2, return_sequences = True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(20, activation=\"relu\"))\n",
    "model.add(Dense(num_actions, activation='softmax'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# history saving\n",
    "reward_chain = []\n",
    "loss_chain = []\n",
    "\n",
    "\n",
    "\n",
    "# Main loop\n",
    "for epoch in range(num_episodes):\n",
    "    loss = 0.\n",
    "    total_reward_by_episode = 0\n",
    "    # Reset enviromet, actualize the data batch\n",
    "    states = env.reset()\n",
    "\n",
    "    done = False\n",
    "\n",
    "    # Define exploration to improve performance\n",
    "    exploration = 1\n",
    "    # Iteration in one episode\n",
    "    q = np.zeros([batch_size,num_actions])\n",
    "    \n",
    "    i_iteration = 0\n",
    "    for i_iteration in range(iterations_episode):\n",
    "    #while not done:\n",
    "        i_iteration += 1\n",
    "\n",
    "        # get next action\n",
    "        if exploration > 0.001:\n",
    "            exploration = epsilon*decay_rate**(epoch*i_iteration)            \n",
    "\n",
    "        if np.random.rand() <= exploration:\n",
    "            actions = np.random.randint(0, num_actions,batch_size)\n",
    "        else:\n",
    "            q = model.predict(states)\n",
    "            actions = np.argmax(q,axis=1)\n",
    "\n",
    "        # apply actions, get rewards and new state\n",
    "        next_states, reward, done, _ = env.step(actions)        \n",
    "        q_prime = model.predict(next_states)\n",
    "\n",
    "        indx = np.argmax(q_prime,axis=1)\n",
    "        sx = np.arange(len(indx))\n",
    "        # Update q values\n",
    "        targets = reward + gamma * q[sx,indx]  \n",
    "        q[sx,actions] = targets\n",
    "\n",
    "        # Train network, update loss\n",
    "        loss += model.train_on_batch(states, q)[0]\n",
    "\n",
    "        # Update the state\n",
    "        states = next_states\n",
    "\n",
    "        total_reward_by_episode += int(sum(reward))\n",
    "\n",
    "    if next_states.shape[0] != batch_size:\n",
    "            break # finished df\n",
    "    reward_chain.append(total_reward_by_episode)    \n",
    "    loss_chain.append(loss)\n",
    "\n",
    "    print(\"\\rEpoch {:03d}/{:03d} | Loss {:4.4f} | Tot reward x episode {:03d} \".format(epoch,\n",
    "          num_episodes ,loss, total_reward_by_episode))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "from sklearn.metrics import classification_report\n",
    "q_prime = model.predict(x_test)\n",
    "predictions = np.argmax(q_prime,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
